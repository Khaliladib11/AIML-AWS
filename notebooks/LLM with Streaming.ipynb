{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "026fd538-251d-454b-bbbc-d744bcdb54a7",
   "metadata": {},
   "source": [
    "# Stream LLM Response "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a0ed8e-e412-4749-ab66-6f8605738eef",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2425294-2f58-47cf-a5ba-56eb90f6effb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import io\n",
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.base_deserializers import StreamDeserializer\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cd4f727-f05b-4353-ac62-a782235e11bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boto3 version: 1.28.40\n",
      "sagemaker version: 2.177.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"boto3 version: {boto3.__version__}\")\n",
    "print(f\"sagemaker version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41294ba8-a11a-4310-ae78-c40ca5110af6",
   "metadata": {},
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e1a1678-acc9-498c-940e-9832534dab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROFILE_NAME = \"PROFILE_NAME\"\n",
    "REGION = \"REGION_NAME\"\n",
    "ENDPOINT_NAME = \"falcon-7b-instruct-streaming-endpoint\"\n",
    "ROLE = \"ROLE_NAME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b23f1cb9-acc7-45e5-bffe-f925c26b0057",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = boto3.session.Session(profile_name=PROFILE_NAME, region_name=REGION)\n",
    "sg_session = Session(boto_session=boto_session)\n",
    "smr = boto_session.client('sagemaker-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5224ecb9-e7ca-4e2f-9226-4bfdf0c8742a",
   "metadata": {},
   "source": [
    "## Deploy Falcon Model on Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ea12199-0aac-4243-ba9f-896aa0edb732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TGI Container: 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi0.9.3-gpu-py39-cu118-ubuntu20.04\n"
     ]
    }
   ],
   "source": [
    "# get the huggingface llm image\n",
    "llm_img = get_huggingface_llm_image_uri(\n",
    "    backend=\"huggingface\",\n",
    "    session=sg_session,\n",
    "    version=\"0.9.3\"\n",
    ")\n",
    "\n",
    "print(f\"TGI Container: {llm_img}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "962dcd52-5fb5-4c9f-8647-c49208b71d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model deployment configuration\n",
    "deploy_config = {\n",
    "    'HF_MODEL_ID': \"tiiuae/falcon-7b-instruct\", # model_id from hf.co/models\n",
    "    'SM_NUM_GPUS': json.dumps(1), # Number of GPU used per replica\n",
    "    'MAX_INPUT_LENGTH': json.dumps(3072),  # Max length of input text\n",
    "    'MAX_TOTAL_TOKENS': json.dumps(4096),  # Max length of the generation (including input text)\n",
    "    'MAX_BATCH_TOTAL_TOKENS': json.dumps(8192),  # Limits the number of tokens that can be processed in parallel during the generation\n",
    "    #'HF_MODEL_QUANTIZE': \"bitsandbytes\", # comment in to quantize\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5b7dc1a-bb7b-4db7-9e54-d7a66ce885b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "    role=ROLE,\n",
    "    image_uri=llm_img,\n",
    "    env=deploy_config,\n",
    "    sagemaker_session=sg_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d2aab14-ce9f-4ac9-aa48-20c1ccdc65a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "# Deploy model to an endpoint\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "health_check_timeout = 300\n",
    "\n",
    "llm_endpoint = llm_model.deploy(\n",
    "    endpoint_name=ENDPOINT_NAME,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n",
    "    deserializer=StreamDeserializer()\n",
    ")\n",
    "\n",
    "# llm_endpoint.deserializer=StreamDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e1dd49-93d9-4b28-919b-474ebd39f5e3",
   "metadata": {},
   "source": [
    "## Helper Classes and Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe626d0b-13f7-4603-aba2-fcc520cb7887",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineIterator:\n",
    "    \"\"\"\n",
    "    A helper class for parsing the byte stream input. \n",
    "    \n",
    "    The output of the model will be in the following format:\n",
    "    ```\n",
    "    b'{\"outputs\": [\" a\"]}\\n'\n",
    "    b'{\"outputs\": [\" challenging\"]}\\n'\n",
    "    b'{\"outputs\": [\" problem\"]}\\n'\n",
    "    ...\n",
    "    ```\n",
    "    \n",
    "    While usually each PayloadPart event from the event stream will contain a byte array \n",
    "    with a full json, this is not guaranteed and some of the json objects may be split across\n",
    "    PayloadPart events. For example:\n",
    "    ```\n",
    "    {'PayloadPart': {'Bytes': b'{\"outputs\": '}}\n",
    "    {'PayloadPart': {'Bytes': b'[\" problem\"]}\\n'}}\n",
    "    ```\n",
    "    \n",
    "    This class accounts for this by concatenating bytes written via the 'write' function\n",
    "    and then exposing a method which will return lines (ending with a '\\n' character) within\n",
    "    the buffer via the 'scan_lines' function. It maintains the position of the last read \n",
    "    position to ensure that previous bytes are not exposed again. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, stream):\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "            if line and line[-1] == ord('\\n'):\n",
    "                self.read_pos += len(line)\n",
    "                return line[:-1]\n",
    "            try:\n",
    "                chunk = next(self.byte_iterator)\n",
    "            except StopIteration:\n",
    "                if self.read_pos < self.buffer.getbuffer().nbytes:\n",
    "                    continue\n",
    "                raise\n",
    "            if 'PayloadPart' not in chunk:\n",
    "                print('Unknown event type:' + chunk)\n",
    "                continue\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk['PayloadPart']['Bytes'])\n",
    "            \n",
    "\n",
    "def invoke_stream_endpoint(endpoint_name, query, stop_token=\"<|endoftext|>\"):\n",
    "    body = {\n",
    "        \"inputs\": query,\n",
    "        \"parameters\":{\n",
    "            \"max_new_tokens\":400,\n",
    "            \"return_full_text\": False\n",
    "        },\n",
    "        \"stream\": True\n",
    "    }\n",
    "    \n",
    "    \n",
    "    llm_endpoint.deserializer=StreamDeserializer()\n",
    "    resp = smr.invoke_endpoint_with_response_stream(\n",
    "        EndpointName=endpoint_name, \n",
    "        Body=json.dumps(body), \n",
    "        ContentType='application/json')\n",
    "    \n",
    "    event_stream = resp['Body']\n",
    "    start_json = b'{'\n",
    "    for line in LineIterator(event_stream):\n",
    "        if line != b'' and start_json in line:\n",
    "            data = json.loads(line[line.find(start_json):].decode('utf-8'))\n",
    "            if data['token']['text'] != stop_token:\n",
    "                print(data['token']['text'],end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58ec6a4-2013-40b5-ab4b-22e3dbdccaec",
   "metadata": {},
   "source": [
    "## Test the Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "244b996f-5259-4ba7-8fb5-64ff71b14ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sure! Here are 10 recipes that are healthy and delicious:\n",
      "1. Grilled salmon with lemon and capers\n",
      "2. Quinoa and vegetable stir-fry\n",
      "3. Baked sweet potato with black beans and salsa\n",
      "4. Chicken and vegetable soup\n",
      "5. Egg and vegetable omelette\n",
      "6. Greek yogurt and berry parfait\n",
      "7. Grilled chicken with avocado and tomato\n",
      "8. Lentil soup with spinach and carrots\n",
      "9. Roasted vegetables with chickpeas\n",
      "10. Turkey and vegetable chili"
     ]
    }
   ],
   "source": [
    "invoke_stream_endpoint(\n",
    "    endpoint_name=ENDPOINT_NAME, \n",
    "    query=\"How to cook good meal for diet? can you give me a list of 10 recipes?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffa78ba9-d39a-4ead-8d34-5c79ee0bdbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sure! Here are some of the best places to visit in London:\n",
      "1. Tower of London\n",
      "2. Big Ben\n",
      "3. London Eye\n",
      "4. Buckingham Palace\n",
      "5. Trafalgar Square\n",
      "6. Hyde Park\n",
      "7. The British Museum\n",
      "8. National Gallery\n",
      "9. Tate Modern\n",
      "10. St. Paul's Cathedral"
     ]
    }
   ],
   "source": [
    "invoke_stream_endpoint(\n",
    "    endpoint_name=ENDPOINT_NAME, \n",
    "    query=\"Can you give me best 10 places to visit in London please?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e32f518-ee41-4beb-a247-ba3c1a1eee52",
   "metadata": {},
   "source": [
    "## Kill Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9263362e-0129-4997-9aec-d8489fa6fd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_endpoint.delete_model()\n",
    "llm_endpoint.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
